{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use our deep learning models to predict recurrence in cancer patients using time-series data. Due to the sensitivity of medical data, we are unable to release the real dataset. Instead, we demonstrate the use of our models on synthetic time-series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Bidirectional, Input, Flatten, Concatenate, RepeatVector, TimeDistributed, Conv1D, Layer, LayerNormalization, GlobalAveragePooling1D, LSTM\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K \n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset.  \n",
    "Our method for generating synthetic data is described in https://github.com/phu5ion/colorectal-prognostication/tree/master/notebooks/create_simul_data.ipynb. However any method can be used as long as synthetic data is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamps</th>\n",
       "      <th>cea</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>0.359474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>0.485895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>0.871115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-31</td>\n",
       "      <td>0.871115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>1.663848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamps       cea  patient_id\n",
       "0  2020-08-01  0.359474           1\n",
       "1  2020-09-01  0.485895           1\n",
       "2  2020-10-01  0.871115           1\n",
       "3  2020-10-31  0.871115           1\n",
       "4  2020-12-01  1.663848           1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = pd.read_csv(\"synthetic_ts.csv\")\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamps</th>\n",
       "      <th>cea</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>normed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>0.359474</td>\n",
       "      <td>1</td>\n",
       "      <td>0.034344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>0.485895</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>0.871115</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-31</td>\n",
       "      <td>0.871115</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>1.663848</td>\n",
       "      <td>1</td>\n",
       "      <td>0.168124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamps       cea  patient_id    normed\n",
       "0  2020-08-01  0.359474           1  0.034344\n",
       "1  2020-09-01  0.485895           1  0.047310\n",
       "2  2020-10-01  0.871115           1  0.086819\n",
       "3  2020-10-31  0.871115           1  0.086819\n",
       "4  2020-12-01  1.663848           1  0.168124"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalise CEA values\n",
    "cea = ts['cea'].to_numpy().reshape(-1, 1)\n",
    "s = MinMaxScaler()\n",
    "out = s.fit_transform(cea)\n",
    "ts['normed'] = out\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n"
     ]
    }
   ],
   "source": [
    "# Next reshape into list of normalised CEA\n",
    "out = []\n",
    "patrow = []\n",
    "patid = 1\n",
    "for row in ts.iterrows():\n",
    "    pat = row[1]['patient_id']\n",
    "    cea = row[1]['normed']\n",
    "    if pat != patid:\n",
    "        out.append(patrow)\n",
    "        patrow = []\n",
    "        patid = pat\n",
    "    else:\n",
    "        patrow.append(cea)\n",
    "out.append(patrow) # Append last row\n",
    "print(len(out)) # 900 patients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Then we pad the dataset\n",
    "dataset = tf.keras.preprocessing.sequence.pad_sequences(out, maxlen=None, dtype='float64', padding='pre', truncating='pre')\n",
    "dataset = np.expand_dims(dataset, axis=-1) # Because we only have 1 feature, we add an extra dimension to fit what model expects\n",
    "print(dataset.shape)\n",
    "ts_length = dataset.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels from the tabular data\n",
    "tabular = pd.read_csv(\"synthetic_structured.csv\")\n",
    "labels = tabular['relapse'].to_numpy()\n",
    "labels = np.where(labels=='yes', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((576, 32, 1), (144, 32, 1), (180, 32, 1))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do train-val-test split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(dataset, labels, test_size=0.2, random_state=100)\n",
    "xtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.2, random_state=100)\n",
    "xtrain.shape, xval.shape, xtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load models/htmv.py\n",
    "# Code for positional encodings\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8, future_mask=None, use_cnn=True):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads # query, value and key vector dim will be embedding dim // num_heads\n",
    "         \n",
    "        if use_cnn:\n",
    "            self.query_dense = Conv1D(embed_dim, kernel_size=3, padding=\"causal\")\n",
    "            self.key_dense = Conv1D(embed_dim, kernel_size=3, padding=\"causal\")\n",
    "            self.value_dense = Conv1D(embed_dim, kernel_size=3, padding=\"causal\")\n",
    "        else:\n",
    "            self.query_dense = Dense(embed_dim)\n",
    "            self.key_dense = Dense(embed_dim)\n",
    "            self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "        self.future_mask = future_mask\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        if mask is not None:\n",
    "            scaled_score += (mask * -1e9)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) # transposes along dimension 0 (the batch)\n",
    "\n",
    "    def call(self, inputs): # Query, key and value are the input tensors to attention model\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        query_input, key_input, value_input = inputs\n",
    "        batch_size = tf.shape(query_input)[0]\n",
    "        # Initialise query, key and value vectors\n",
    "        query = self.query_dense(query_input)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(key_input)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(value_input)  # (batch_size, seq_len, embed_dim)\n",
    "        # Separate out an extra dim by num_heads. So calculation is done separately for each head. \n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        # Perform the attention calculation\n",
    "        attention, weights = self.attention(query, key, value, self.future_mask)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim) # Do the transpose to make it easier to combine the last two dimensions later on \n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim) # embed_dim = projection_dim * num_heads\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim) # do a linear projection\n",
    "        return output\n",
    "def transformer(d_model, ts_length, stack=2, h=8, dropout=0.1, local_att_size=10, mask='local', use_cnn=True):\n",
    "    # stack: num encoder and decoder stacks, each\n",
    "    # h: num heads. h>1 for multi-headed attention\n",
    "    kernel_init='he_uniform'\n",
    "    \n",
    "    ts_input = Input(shape=(ts_length, 1))\n",
    "    # Positional encodings\n",
    "    pos_encodings = positional_encoding(ts_length, d_model)\n",
    "    # Embedding\n",
    "    embedding = Dense(d_model, activation='linear', name='original_encodings')(ts_input + pos_encodings) # Linear encoding # Dense layer with 1dim is actually the same as an Embedding layer\n",
    "    # Create look_ahead_mask for causal attention. This works on the global sequence\n",
    "    seq_len = ts_length\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) # Set lower triangular part to 1, upper to 0\n",
    "    # We also create a local mask that also implements causal attention so that it doesn't need to look so far behind\n",
    "    local_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), local_att_size, 0)\n",
    "    # We train this as a stack of attentional layers with causal masking\n",
    "    x = embedding\n",
    "    \n",
    "    # Which mask to use?\n",
    "    if mask=='local':\n",
    "        mask = local_mask\n",
    "    else:\n",
    "        mask = look_ahead_mask\n",
    "    # Encoder\n",
    "    for i in range(stack):\n",
    "        # This is 1 transformer block\n",
    "        # Self attention\n",
    "        residual = x\n",
    "        x = MultiHeadSelfAttention(embed_dim=d_model, num_heads=h, future_mask=None, use_cnn=use_cnn)([x,x,x]) # Passing in list of query, key and value inputs for self-attention\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x+residual)\n",
    "        # Feed-forward\n",
    "        residual = x\n",
    "        x = Sequential([Dense(d_model, activation=\"relu\", kernel_initializer=kernel_init), Dense(d_model),])(x) # feed-forward layer with relu in between\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x+residual)\n",
    "    enc_output = x # We save this in our memory\n",
    "    # Decoder\n",
    "    for i in range(stack):\n",
    "        # This is 1 transformer block\n",
    "        # Self attention\n",
    "        residual = x\n",
    "        x = MultiHeadSelfAttention(embed_dim=d_model, num_heads=h, future_mask=mask, use_cnn=use_cnn)([x,x,x]) \n",
    "        x = Dropout(dropout)(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x+residual)\n",
    "        # Encoder-decoder attention\n",
    "        residual = x\n",
    "        x = MultiHeadSelfAttention(embed_dim=d_model, num_heads=h, use_cnn=use_cnn)([x,enc_output,enc_output]) # key and value are our encoder output/memory\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x+residual)\n",
    "        # Feed-forward\n",
    "        residual = x\n",
    "        x = Sequential([Dense(d_model, activation=\"relu\", kernel_initializer=kernel_init), Dense(d_model),])(x) # feed-forward layer with relu in between\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x+residual)\n",
    "        \n",
    "    # Apply dense layer\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(20, activation='relu', kernel_initializer=kernel_init)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\", )(x)\n",
    "\n",
    "    model = Model(inputs=ts_input, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load models/dl_models.py\n",
    "# LSTM\n",
    "def create_masking_model(ts_length, bidir=1):\n",
    "    ts_input = Input((ts_length, 1))\n",
    "    kernel_regularizer=L1L2(l1=0.01, l2=0.01)\n",
    "    # LSTM-input\n",
    "    mask = tf.keras.layers.Masking(mask_value=0.0)(ts_input)\n",
    "    if bidir:\n",
    "        lstm_layer=Bidirectional(LSTM(8, return_sequences=True, dropout = 0.2, recurrent_dropout=0.2, activation='tanh', kernel_regularizer=kernel_regularizer, kernel_initializer=\"glorot_uniform\"))(mask)\n",
    "    else:\n",
    "        lstm_layer=LSTM(16, return_sequences=True, dropout = 0.2, recurrent_dropout=0.2, activation='tanh', kernel_regularizer=kernel_regularizer, kernel_initializer=\"glorot_uniform\")(mask)\n",
    "    lstm_layer=LSTM(8, dropout = 0.2, activation='tanh', recurrent_dropout=0.2, kernel_regularizer=kernel_regularizer, kernel_initializer=\"glorot_uniform\")(lstm_layer)\n",
    "    output=Dense(1, activation='sigmoid')(lstm_layer)\n",
    "    # define a model with a list of two inputs\n",
    "    model = Model(inputs=ts_input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Temporal Convolutional Network (TCN aka Wavenet)\n",
    "def create_tcn(num_channels, ts_length, kernel_size=2, strides=1, dropout=0.1):\n",
    "    # Initialise required stuff\n",
    "    ts_input = Input(shape=(ts_length, 1))\n",
    "    kernel_regularizer=L1L2(l1=0.00, l2=0.00)\n",
    "    kernel_initializer=\"he_uniform\" # Instead of glorot_uniform as he_uniform seems to be theoretically better for relu and relu-like activations\n",
    "    lnorm=LayerNormalization() # LayerNorm and BatchNorm both doesn't work...?\n",
    "    # Depending on the number of levels, we increase dilation rate.\n",
    "    # Number of levels should be self-calculated...\n",
    "    num_levels = len(num_channels) # It should look like a list of length levels, how many filters each level\n",
    "    inputs = ts_input\n",
    "    for i in range(num_levels):\n",
    "        dilation_size = 2 ** i\n",
    "        out_channels = num_channels[i]\n",
    "        \n",
    "        # This is 1 block\n",
    "        cnn1 = Conv1D(filters=out_channels, kernel_size=kernel_size, strides=strides, padding='causal', data_format='channels_last', activation='relu', dilation_rate=dilation_size, kernel_initializer=kernel_initializer)(inputs)\n",
    "        #norm1 = lnorm(cnn1)\n",
    "        dropout1 = Dropout(dropout, noise_shape=[tf.constant(1), tf.constant(1), tf.constant(out_channels)])(cnn1) # Noise_shape is to apply uniform dropout to all timesteps\n",
    "        cnn2 = Conv1D(filters=out_channels, kernel_size=kernel_size, strides=strides, padding='causal', data_format='channels_last', activation='relu', dilation_rate=dilation_size, kernel_initializer=kernel_initializer)(dropout1)\n",
    "        #norm2 = lnorm(cnn2)\n",
    "        dropout2 = Dropout(dropout, noise_shape=[tf.constant(1), tf.constant(1), tf.constant(out_channels)])(cnn2) # Noise_shape is to apply uniform dropout to all timesteps\n",
    "        out = relu(dropout2 + inputs) # Skip connections\n",
    "        inputs = out\n",
    "            \n",
    "    out = out[:, -1, :]\n",
    "    output = Dense(1, activation='sigmoid')(out)\n",
    "    \n",
    "    # define a model with a list of two inputs\n",
    "    model = Model(inputs=ts_input, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some metrics\n",
    "auc = tf.keras.metrics.AUC()\n",
    "prec = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "trueneg = tf.keras.metrics.TrueNegatives()\n",
    "# Implement my own Balanced Acc calculation\n",
    "def balanced_acc(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        # recall of class 1\n",
    "\n",
    "        #do not use \"round\" here if you're going to use this as a loss function\n",
    "        true_positives = K.sum(K.round(y_pred) * y_true)\n",
    "        possible_positives = K.sum(y_true)\n",
    "        return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    return (recall(y_true,y_pred) + recall(1-y_true,1-y_pred))/2.\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(modelname, xtrain, xval, ytrain, yval, batch_size, epoches, filename, verbose=1): \n",
    "    if modelname == 'transformer':\n",
    "        model = transformer(d_model=64, ts_length=ts_length, stack=1, dropout=0.1, h=8, local_att_size=10)\n",
    "    elif modelname == 'lstm':\n",
    "        model = create_masking_model(ts_length=ts_length, bidir=1)\n",
    "    elif modelname == 'tcn':\n",
    "        layers=6\n",
    "        model = create_tcn([60]*layers, ts_length=ts_length, kernel_size=2, dropout=0.1) # 60 hidden nodes, 6 levels\n",
    "    opt = Adam(clipvalue=5, lr=0.0001) \n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[balanced_acc, recall, prec, specificity])\n",
    "    history = model.fit(xtrain, ytrain, batch_size=batch_size, epochs=epoches, verbose=verbose, validation_data=[xval, yval])\n",
    "    # Save model\n",
    "    model.save_weights(filename+\"/\"+filename+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 144 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 3s 6ms/sample - loss: 1.6952 - balanced_acc: 0.5077 - recall: 0.9315 - precision: 0.2731 - specificity: 0.1384 - val_loss: 0.5782 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 801us/sample - loss: 0.6590 - balanced_acc: 0.5102 - recall: 0.1233 - precision: 0.3051 - specificity: 0.9019 - val_loss: 0.6040 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 779us/sample - loss: 0.6579 - balanced_acc: 0.5092 - recall: 0.1438 - precision: 0.2763 - specificity: 0.8746 - val_loss: 0.6207 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 783us/sample - loss: 0.7178 - balanced_acc: 0.5049 - recall: 0.1027 - precision: 0.2344 - specificity: 0.8908 - val_loss: 0.5710 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 790us/sample - loss: 0.6375 - balanced_acc: 0.5104 - recall: 0.0479 - precision: 0.2000 - specificity: 0.9425 - val_loss: 0.5665 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 854us/sample - loss: 0.6554 - balanced_acc: 0.4952 - recall: 0.0685 - precision: 0.2500 - specificity: 0.9287 - val_loss: 0.5617 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 797us/sample - loss: 0.6287 - balanced_acc: 0.5013 - recall: 0.1370 - precision: 0.2500 - specificity: 0.8579 - val_loss: 0.5651 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 834us/sample - loss: 0.6233 - balanced_acc: 0.5065 - recall: 0.0685 - precision: 0.2857 - specificity: 0.9425 - val_loss: 0.5685 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 829us/sample - loss: 0.6484 - balanced_acc: 0.5106 - recall: 0.1781 - precision: 0.3333 - specificity: 0.8721 - val_loss: 0.5644 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 833us/sample - loss: 0.6065 - balanced_acc: 0.5069 - recall: 0.1096 - precision: 0.3019 - specificity: 0.9116 - val_loss: 0.5673 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 834us/sample - loss: 0.6003 - balanced_acc: 0.5119 - recall: 0.0548 - precision: 0.4444 - specificity: 0.9759 - val_loss: 0.5684 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 819us/sample - loss: 0.5856 - balanced_acc: 0.5276 - recall: 0.1164 - precision: 0.4474 - specificity: 0.9490 - val_loss: 0.5662 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 827us/sample - loss: 0.5871 - balanced_acc: 0.5023 - recall: 0.0479 - precision: 0.2692 - specificity: 0.9514 - val_loss: 0.5577 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 837us/sample - loss: 0.6134 - balanced_acc: 0.5221 - recall: 0.1712 - precision: 0.3333 - specificity: 0.8806 - val_loss: 0.5613 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 851us/sample - loss: 0.6182 - balanced_acc: 0.5031 - recall: 0.0342 - precision: 0.1923 - specificity: 0.9563 - val_loss: 0.5598 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 844us/sample - loss: 0.5839 - balanced_acc: 0.5012 - recall: 0.0342 - precision: 0.2000 - specificity: 0.9579 - val_loss: 0.5596 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 1s 893us/sample - loss: 0.6555 - balanced_acc: 0.5389 - recall: 0.1849 - precision: 0.3699 - specificity: 0.8951 - val_loss: 0.5614 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 1s 883us/sample - loss: 0.6147 - balanced_acc: 0.5042 - recall: 0.0959 - precision: 0.3415 - specificity: 0.9306 - val_loss: 0.5593 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 865us/sample - loss: 0.5658 - balanced_acc: 0.5089 - recall: 0.0342 - precision: 0.5000 - specificity: 0.9874 - val_loss: 0.5548 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 866us/sample - loss: 0.5867 - balanced_acc: 0.5057 - recall: 0.0411 - precision: 0.2400 - specificity: 0.9597 - val_loss: 0.5557 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 1s 933us/sample - loss: 0.5904 - balanced_acc: 0.5438 - recall: 0.1370 - precision: 0.4082 - specificity: 0.9350 - val_loss: 0.5557 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 851us/sample - loss: 0.5885 - balanced_acc: 0.5037 - recall: 0.1027 - precision: 0.2885 - specificity: 0.9124 - val_loss: 0.5593 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 865us/sample - loss: 0.5966 - balanced_acc: 0.5100 - recall: 0.0890 - precision: 0.3514 - specificity: 0.9428 - val_loss: 0.5620 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 1s 928us/sample - loss: 0.5615 - balanced_acc: 0.5178 - recall: 0.0822 - precision: 0.4000 - specificity: 0.9553 - val_loss: 0.5592 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 1s 874us/sample - loss: 0.5744 - balanced_acc: 0.5031 - recall: 0.0068 - precision: 1.0000 - specificity: 1.0000 - val_loss: 0.5543 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 1s 945us/sample - loss: 0.5747 - balanced_acc: 0.5075 - recall: 0.0890 - precision: 0.3095 - specificity: 0.9311 - val_loss: 0.5526 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 1s 906us/sample - loss: 0.5679 - balanced_acc: 0.5179 - recall: 0.0753 - precision: 0.4583 - specificity: 0.9692 - val_loss: 0.5544 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 1s 882us/sample - loss: 0.5744 - balanced_acc: 0.5081 - recall: 0.0479 - precision: 0.4375 - specificity: 0.9773 - val_loss: 0.5572 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 1s 872us/sample - loss: 0.5828 - balanced_acc: 0.4948 - recall: 0.0068 - precision: 0.1250 - specificity: 0.9834 - val_loss: 0.5536 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 1s 909us/sample - loss: 0.5867 - balanced_acc: 0.4964 - recall: 0.0137 - precision: 0.1538 - specificity: 0.9766 - val_loss: 0.5527 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 1s 912us/sample - loss: 0.5760 - balanced_acc: 0.5053 - recall: 0.0411 - precision: 0.2143 - specificity: 0.9534 - val_loss: 0.5546 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 1s 923us/sample - loss: 0.5579 - balanced_acc: 0.5106 - recall: 0.0205 - precision: 0.7500 - specificity: 0.9981 - val_loss: 0.5517 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 1s 939us/sample - loss: 0.5746 - balanced_acc: 0.5108 - recall: 0.0205 - precision: 0.4286 - specificity: 0.9916 - val_loss: 0.5547 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 1s 963us/sample - loss: 0.5692 - balanced_acc: 0.5138 - recall: 0.1233 - precision: 0.3273 - specificity: 0.9118 - val_loss: 0.5517 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 1s 937us/sample - loss: 0.5584 - balanced_acc: 0.5305 - recall: 0.0959 - precision: 0.5185 - specificity: 0.9692 - val_loss: 0.5508 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 1s 912us/sample - loss: 0.5537 - balanced_acc: 0.5058 - recall: 0.0068 - precision: 0.5000 - specificity: 0.9978 - val_loss: 0.5518 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 1s 955us/sample - loss: 0.5500 - balanced_acc: 0.5009 - recall: 0.0205 - precision: 0.2727 - specificity: 0.9809 - val_loss: 0.5524 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 1s 939us/sample - loss: 0.5703 - balanced_acc: 0.5188 - recall: 0.0411 - precision: 0.5000 - specificity: 0.9865 - val_loss: 0.5555 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 1s 928us/sample - loss: 0.5636 - balanced_acc: 0.5162 - recall: 0.0685 - precision: 0.4762 - specificity: 0.9725 - val_loss: 0.5578 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 1s 919us/sample - loss: 0.5901 - balanced_acc: 0.5258 - recall: 0.0616 - precision: 0.2903 - specificity: 0.9548 - val_loss: 0.5573 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 1s 962us/sample - loss: 0.5725 - balanced_acc: 0.5003 - recall: 0.0205 - precision: 0.2143 - specificity: 0.9770 - val_loss: 0.5593 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 1s 936us/sample - loss: 0.5512 - balanced_acc: 0.5180 - recall: 0.0548 - precision: 0.4706 - specificity: 0.9798 - val_loss: 0.5582 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 1s 947us/sample - loss: 0.5566 - balanced_acc: 0.5104 - recall: 0.0411 - precision: 0.4286 - specificity: 0.9813 - val_loss: 0.5583 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 1s 964us/sample - loss: 0.5534 - balanced_acc: 0.4977 - recall: 0.0000e+00 - precision: 0.0000e+00 - specificity: 0.9954 - val_loss: 0.5588 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 1s 925us/sample - loss: 0.5608 - balanced_acc: 0.5001 - recall: 0.0342 - precision: 0.2381 - specificity: 0.9631 - val_loss: 0.5580 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 1s 902us/sample - loss: 0.5317 - balanced_acc: 0.5079 - recall: 0.0342 - precision: 0.5000 - specificity: 0.9865 - val_loss: 0.5586 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 1s 874us/sample - loss: 0.5320 - balanced_acc: 0.5017 - recall: 0.0342 - precision: 0.3571 - specificity: 0.9770 - val_loss: 0.5585 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 1s 930us/sample - loss: 0.5768 - balanced_acc: 0.5107 - recall: 0.0274 - precision: 0.1481 - specificity: 0.9566 - val_loss: 0.5594 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 1s 895us/sample - loss: 0.5463 - balanced_acc: 0.5000 - recall: 0.0068 - precision: 0.2500 - specificity: 0.9931 - val_loss: 0.5581 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 1s 906us/sample - loss: 0.5442 - balanced_acc: 0.5023 - recall: 0.0068 - precision: 1.0000 - specificity: 1.0000 - val_loss: 0.5589 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 1s 921us/sample - loss: 0.5711 - balanced_acc: 0.5041 - recall: 0.0274 - precision: 0.2500 - specificity: 0.9733 - val_loss: 0.5595 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 1s 879us/sample - loss: 0.5305 - balanced_acc: 0.5023 - recall: 0.0068 - precision: 0.5000 - specificity: 0.9977 - val_loss: 0.5594 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 1s 919us/sample - loss: 0.5354 - balanced_acc: 0.5017 - recall: 0.0068 - precision: 0.5000 - specificity: 0.9978 - val_loss: 0.5594 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 1s 882us/sample - loss: 0.5462 - balanced_acc: 0.4956 - recall: 0.0000e+00 - precision: 0.0000e+00 - specificity: 0.9912 - val_loss: 0.5590 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 1s 914us/sample - loss: 0.5408 - balanced_acc: 0.5131 - recall: 0.0411 - precision: 0.5455 - specificity: 0.9872 - val_loss: 0.5585 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 864us/sample - loss: 0.5561 - balanced_acc: 0.4979 - recall: 0.0000e+00 - precision: 0.0000e+00 - specificity: 0.9957 - val_loss: 0.5587 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 1s 923us/sample - loss: 0.5480 - balanced_acc: 0.5107 - recall: 0.0274 - precision: 0.5000 - specificity: 0.9909 - val_loss: 0.5588 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 853us/sample - loss: 0.5525 - balanced_acc: 0.5095 - recall: 0.0479 - precision: 0.4118 - specificity: 0.9758 - val_loss: 0.5573 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 1s 891us/sample - loss: 0.5457 - balanced_acc: 0.5200 - recall: 0.0479 - precision: 0.6364 - specificity: 0.9910 - val_loss: 0.5569 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 1s 897us/sample - loss: 0.5331 - balanced_acc: 0.5129 - recall: 0.0205 - precision: 0.7500 - specificity: 0.9979 - val_loss: 0.5558 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 815us/sample - loss: 0.5293 - balanced_acc: 0.5001 - recall: 0.0137 - precision: 0.2857 - specificity: 0.9870 - val_loss: 0.5563 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 861us/sample - loss: 0.5366 - balanced_acc: 0.4965 - recall: 0.0274 - precision: 0.2667 - specificity: 0.9710 - val_loss: 0.5567 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 1s 883us/sample - loss: 0.5345 - balanced_acc: 0.4979 - recall: 0.0000e+00 - precision: 0.0000e+00 - specificity: 0.9958 - val_loss: 0.5569 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 868us/sample - loss: 0.5239 - balanced_acc: 0.5176 - recall: 0.0411 - precision: 0.7500 - specificity: 0.9949 - val_loss: 0.5567 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 1s 877us/sample - loss: 0.5604 - balanced_acc: 0.5033 - recall: 0.0205 - precision: 0.3000 - specificity: 0.9846 - val_loss: 0.5541 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 1s 872us/sample - loss: 0.5405 - balanced_acc: 0.5069 - recall: 0.0137 - precision: 1.0000 - specificity: 1.0000 - val_loss: 0.5544 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 865us/sample - loss: 0.5698 - balanced_acc: 0.5060 - recall: 0.0479 - precision: 0.2414 - specificity: 0.9522 - val_loss: 0.5544 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 866us/sample - loss: 0.5595 - balanced_acc: 0.5137 - recall: 0.0205 - precision: 0.6000 - specificity: 0.9959 - val_loss: 0.5574 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 859us/sample - loss: 0.5535 - balanced_acc: 0.5160 - recall: 0.0548 - precision: 0.3636 - specificity: 0.9690 - val_loss: 0.5553 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 867us/sample - loss: 0.5298 - balanced_acc: 0.5118 - recall: 0.0274 - precision: 1.0000 - specificity: 1.0000 - val_loss: 0.5555 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 810us/sample - loss: 0.5473 - balanced_acc: 0.5109 - recall: 0.0205 - precision: 0.6000 - specificity: 0.9956 - val_loss: 0.5541 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 1s 882us/sample - loss: 0.5446 - balanced_acc: 0.5154 - recall: 0.0274 - precision: 0.3636 - specificity: 0.9855 - val_loss: 0.5545 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 1s 991us/sample - loss: 0.5240 - balanced_acc: 0.5160 - recall: 0.0342 - precision: 1.0000 - specificity: 1.0000 - val_loss: 0.5547 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 1s 983us/sample - loss: 0.5147 - balanced_acc: 0.5077 - recall: 0.0205 - precision: 0.6000 - specificity: 0.9951 - val_loss: 0.5566 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 1s 997us/sample - loss: 0.5299 - balanced_acc: 0.5088 - recall: 0.0137 - precision: 0.6667 - specificity: 0.9976 - val_loss: 0.5569 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 1s 1ms/sample - loss: 0.5296 - balanced_acc: 0.5011 - recall: 0.0137 - precision: 0.2857 - specificity: 0.9891 - val_loss: 0.5564 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 1s 1ms/sample - loss: 0.5358 - balanced_acc: 0.5068 - recall: 0.0411 - precision: 0.3529 - specificity: 0.9745 - val_loss: 0.5560 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 1s 967us/sample - loss: 0.5220 - balanced_acc: 0.5097 - recall: 0.0205 - precision: 0.7500 - specificity: 0.9979 - val_loss: 0.5573 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 1s 976us/sample - loss: 0.5278 - balanced_acc: 0.5039 - recall: 0.0274 - precision: 0.3636 - specificity: 0.9839 - val_loss: 0.5595 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 1s 971us/sample - loss: 0.5237 - balanced_acc: 0.5041 - recall: 0.0205 - precision: 0.3750 - specificity: 0.9891 - val_loss: 0.5591 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 1s 981us/sample - loss: 0.5102 - balanced_acc: 0.5120 - recall: 0.0342 - precision: 0.8333 - specificity: 0.9978 - val_loss: 0.5578 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 1s 975us/sample - loss: 0.5273 - balanced_acc: 0.5092 - recall: 0.0205 - precision: 0.5000 - specificity: 0.9928 - val_loss: 0.5588 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 1s 965us/sample - loss: 0.5222 - balanced_acc: 0.5118 - recall: 0.0479 - precision: 0.4118 - specificity: 0.9764 - val_loss: 0.5595 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 1s 951us/sample - loss: 0.5254 - balanced_acc: 0.5207 - recall: 0.0342 - precision: 0.6250 - specificity: 0.9931 - val_loss: 0.5582 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 1s 964us/sample - loss: 0.5250 - balanced_acc: 0.5350 - recall: 0.0822 - precision: 0.6316 - specificity: 0.9836 - val_loss: 0.5589 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 1s 973us/sample - loss: 0.5243 - balanced_acc: 0.5038 - recall: 0.0137 - precision: 0.2500 - specificity: 0.9876 - val_loss: 0.5587 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 1s 963us/sample - loss: 0.5204 - balanced_acc: 0.5075 - recall: 0.0205 - precision: 0.5000 - specificity: 0.9930 - val_loss: 0.5582 - val_balanced_acc: 0.4950 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 0.9900\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 1s 972us/sample - loss: 0.5262 - balanced_acc: 0.5334 - recall: 0.0753 - precision: 0.6875 - specificity: 0.9884 - val_loss: 0.5565 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 1s 950us/sample - loss: 0.5151 - balanced_acc: 0.5075 - recall: 0.0274 - precision: 0.3636 - specificity: 0.9822 - val_loss: 0.5565 - val_balanced_acc: 0.4889 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 0.9778\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 1s 989us/sample - loss: 0.5243 - balanced_acc: 0.5140 - recall: 0.0411 - precision: 0.5455 - specificity: 0.9884 - val_loss: 0.5554 - val_balanced_acc: 0.4955 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 0.9909\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 848us/sample - loss: 0.5157 - balanced_acc: 0.5003 - recall: 0.0068 - precision: 0.3333 - specificity: 0.9959 - val_loss: 0.5554 - val_balanced_acc: 0.4952 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 0.9905\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 860us/sample - loss: 0.5150 - balanced_acc: 0.5179 - recall: 0.0616 - precision: 0.5000 - specificity: 0.9780 - val_loss: 0.5545 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 1s 868us/sample - loss: 0.5066 - balanced_acc: 0.5112 - recall: 0.0411 - precision: 0.5000 - specificity: 0.9860 - val_loss: 0.5573 - val_balanced_acc: 0.5000 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 1.0000\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 1s 878us/sample - loss: 0.5232 - balanced_acc: 0.5183 - recall: 0.0479 - precision: 0.5385 - specificity: 0.9855 - val_loss: 0.5544 - val_balanced_acc: 0.4960 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 0.9920\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 1s 891us/sample - loss: 0.5080 - balanced_acc: 0.5152 - recall: 0.0411 - precision: 0.5455 - specificity: 0.9889 - val_loss: 0.5533 - val_balanced_acc: 0.4947 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 0.9895\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 1s 916us/sample - loss: 0.5057 - balanced_acc: 0.5154 - recall: 0.0479 - precision: 0.6364 - specificity: 0.9897 - val_loss: 0.5529 - val_balanced_acc: 0.4962 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 0.9923\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 1s 870us/sample - loss: 0.5332 - balanced_acc: 0.5279 - recall: 0.0890 - precision: 0.4643 - specificity: 0.9637 - val_loss: 0.5545 - val_balanced_acc: 0.4957 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 0.9913\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 1s 940us/sample - loss: 0.5072 - balanced_acc: 0.5354 - recall: 0.1027 - precision: 0.5172 - specificity: 0.9677 - val_loss: 0.5569 - val_balanced_acc: 0.4866 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 0.9731\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 1s 973us/sample - loss: 0.5101 - balanced_acc: 0.5282 - recall: 0.0685 - precision: 0.7143 - specificity: 0.9902 - val_loss: 0.5549 - val_balanced_acc: 0.4885 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 0.9770\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 1s 954us/sample - loss: 0.5124 - balanced_acc: 0.5182 - recall: 0.0479 - precision: 0.3889 - specificity: 0.9752 - val_loss: 0.5535 - val_balanced_acc: 0.4911 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_specificity: 0.9822\n"
     ]
    }
   ],
   "source": [
    "train_model('tcn', xtrain, xval, ytrain, yval, batch_size=32, epoches=100, filename='test-tcn', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.7333333333333333, Recall: 0.022222222222222223, Precision: 0.2, Specificity: 0.9703703703703703\n"
     ]
    }
   ],
   "source": [
    "def test_model(xtest, ytest, modelname, modeldir):\n",
    "     # Initialise trained model\n",
    "    if modelname == 'transformer':\n",
    "        newmodel = transformer(d_model=64, ts_length=ts_length, stack=1, dropout=0.1, h=8, local_att_size=10)\n",
    "    elif modelname == 'lstm':\n",
    "        newmodel = create_masking_model(ts_length=ts_length, bidir=1)\n",
    "    elif modelname == 'tcn':\n",
    "        layers=6\n",
    "        newmodel = create_tcn([60]*layers, ts_length=ts_length, kernel_size=2, dropout=0.1) # 60 hidden nodes, 6 levels\n",
    "    # Restore the weights\n",
    "    filename = modeldir+'/'+modeldir+'.ckpt'\n",
    "    newmodel.load_weights(filename).expect_partial()\n",
    "    \n",
    "    y_proba = newmodel.predict(xtest).squeeze()\n",
    "    y_preds = np.round(y_proba)\n",
    "    acc = metrics.accuracy_score(y_true=ytest, y_pred=y_preds)\n",
    "    recall = metrics.recall_score(y_true=ytest, y_pred=y_preds)\n",
    "    precision = metrics.precision_score(y_true=ytest, y_pred=y_preds)\n",
    "    f1 = metrics.f1_score(y_true=ytest, y_pred=y_preds)\n",
    "    bal_acc = metrics.balanced_accuracy_score(y_true=ytest, y_pred=y_preds)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true=ytest, y_pred=y_preds).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    aucpr = metrics.average_precision_score(y_true=ytest, y_score=y_proba)\n",
    "    print(f'Acc: {acc}, Recall: {recall}, Precision: {precision}, Specificity: {specificity}') # You can change whichever metrics you want to be printed out here \n",
    "test_model(xtest, ytest, 'tcn', 'test-tcn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu] *",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
